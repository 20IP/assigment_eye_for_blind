{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EYE FOR BLIND\n",
    "This notebook will be used to prepare the capstone project 'Eye for Blind'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import all the required libraries\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import imutils\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from function import *\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.applications import vgg16\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install imutils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's read the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data understanding\n",
    "1.Import the dataset and read image & captions into two seperate variables\n",
    "\n",
    "2.Visualise both the images & text present in the dataset\n",
    "\n",
    "3.Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "4.Create a list which contains all the captions & path\n",
    "\n",
    "5.Visualise the top 30 occuring words in the captions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset and read the image into a seperate variable\n",
    "\n",
    "data_dir='../data'\n",
    "\n",
    "all_imgs = glob.glob(data_dir + '/Images/*.jpg',recursive=True)\n",
    "print(\"The total images present in the dataset: {}\".format(len(all_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise both the images & text present in the dataset\n",
    "\n",
    "show_img(3,3, all_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the dataset and read the text file into a seperate variable\n",
    "def load_doc():\n",
    "    df_cap = pd.read_csv(data_dir+'/captions.txt', sep = ',')\n",
    "    return df_cap\n",
    "\n",
    "df_cap = load_doc()\n",
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dataframe which summarizes the image, path & captions as a dataframe\n",
    "\n",
    "Each image id has 5 captions associated with it therefore the total dataset should have 40455 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = os.path.abspath(data_dir)+'/Images'\n",
    "all_img_vector = [images_path+'/'+df_cap['image'][i] for i in range(len(df_cap))]\n",
    "all_img_vector[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap['Path'] = all_img_vector\n",
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cap = df_cap[['image', 'Path', 'caption']]\n",
    "df_cap = df_cap.rename(columns={'image': 'ID', 'caption': 'Captions'})\n",
    "\n",
    "df_cap.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a list which contains all the captions\n",
    "df_cap['Captions'] = df_cap['Captions'].apply(lambda x: standardize(x))\n",
    "df_cap['Captions'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total captions present in the dataset: \"+ str(len(df_cap['Captions'])))\n",
    "print(\"Total images present in the dataset: \" + str(len(df_cap['Path'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the vocabulary & the counter for the captions\n",
    "\n",
    "vocabulary=[word.lower() for line in df_cap['Captions'] for word in line.split()]\n",
    "\n",
    "val_count=Counter(vocabulary)\n",
    "val_count.most_common(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualise the top 30 occuring words in the captions\n",
    "\n",
    "most_top_word(val_count, ntop=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show captions and Image\n",
    "show_img_desc(df_cap)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-Processing the captions\n",
    "1.Create the tokenized vectors by tokenizing the captions fore ex :split them using spaces & other filters. \n",
    "This gives us a vocabulary of all of the unique words in the data. Keep the total vocaublary to top 5,000 words for saving memory.\n",
    "\n",
    "2.Replace all other words with the unknown token \"UNK\" .\n",
    "\n",
    "3.Create word-to-index and index-to-word mappings.\n",
    "\n",
    "4.Pad all sequences to be the same length as the longest one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "top_word_cnt = 5000\n",
    "tokenizer = Tokenizer(num_words = top_word_cnt+1, filters= '!\"#$%^&*()_+.,:;-?/~`{}[]|\\=@ ',\n",
    "                      lower = True, char_level = False, \n",
    "                      oov_token = 'UNK')\n",
    "tokenizer.index_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create word-to-index and index-to-word mappings.\n",
    "tokenizer.fit_on_texts(df_cap['Captions'])\n",
    "\n",
    "#transform each text into a sequence of integers\n",
    "text_to_cvt = ['a wide river with many small waterfalls',\n",
    "               'a sled dog pulling a blue toy excavator in the snow']\n",
    "print(tokenizer.texts_to_sequences(text_to_cvt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_seqs = tokenizer.texts_to_sequences(df_cap['Captions'])\n",
    "train_seqs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10 key-value pairs\n",
    "top_10 = {k: tokenizer.index_word[k] for k in list(tokenizer.index_word.keys())[:10]}\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_index['PAD'] = 0\n",
    "tokenizer.index_word[0] = 'PAD'\n",
    "\n",
    "tokenizer.index_word = dict(sorted(tokenizer.index_word.items()))\n",
    "\n",
    "# Get the top 10 key-value pairs\n",
    "top_10 = {k: tokenizer.index_word[k] for k in list(tokenizer.index_word.keys())[:10]}\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.oov_token)\n",
    "print(tokenizer.index_word[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a word count of your tokenizer to visulize the Top 30 occuring words after text processing\n",
    "tokenizer_top_words = [word for line in df_cap['Captions'] for word in line.split() ]\n",
    "\n",
    "#tokenizer_top_words_count\n",
    "tokenizer_top_words_count = Counter(tokenizer_top_words)\n",
    "tokenizer_top_words_count.most_common(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_top_word(tokenizer_top_words_count, ntop=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad each vector to the max_length of the captions ^ store it to a vairable\n",
    "\n",
    "train_seqs_len = [len(seq) for seq in train_seqs]\n",
    "longest_word_length = max(train_seqs_len)\n",
    "cap_vector= tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding= 'post', maxlen = longest_word_length,\n",
    "                                                          dtype='int32', value=0)\n",
    "\n",
    "print(\"The shape of Caption vector is :\" + str(cap_vector.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_cap['Captions'][0])\n",
    "print(train_seqs[0])\n",
    "print(cap_vector[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing the images\n",
    "\n",
    "1.Resize them into the shape of (299, 299)\n",
    "\n",
    "3.Normalize the image within the range of -1 to 1, such that it is in correct format for InceptionV3. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FAQs on how to resize the images::\n",
    "* Since you have a list which contains all the image path, you need to first convert them to a dataset using <i>tf.data.Dataset.from_tensor_slices</i>. Once you have created a dataset consisting of image paths, you need to apply a function to the dataset which will apply the necessary preprocessing to each image. \n",
    "* This function should resize them and also should do the necessary preprocessing that it is in correct format for InceptionV3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, (299, 299))\n",
    "    image = preprocess_input(image)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    return image, image_path\n",
    "\n",
    "@tf.function\n",
    "def get_image_features(images):\n",
    "    # preprocessed_images = tf.keras.applications.inception_v3.preprocess_input(images)\n",
    "    # features = inception_model(images, training=False)\n",
    "    features = tf.map_fn(lambda x: inception_model(x, training=False), images)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '../data/Images/10815824_2997e03d76.jpg'\n",
    "features_test, _ = load_image(img_path)\n",
    "print(features_test.shape)\n",
    "# feat = get_image_features([features_test])\n",
    "# feat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_proc_list = tf.data.Dataset.from_tensor_slices(df_cap['Path'].unique())\n",
    "img_proc_list = img_proc_list.map(load_image, num_parallel_calls = tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "img_proc_list = img_proc_list.batch(16, drop_remainder=False)\n",
    "img_proc_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchIMG, pathDir = next(iter(img_proc_list))\n",
    "pathDir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_path = pathDir[0].numpy().decode('utf-8')\n",
    "# feature_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batchIMG[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(1,5)\n",
    "figure.set_figwidth(25)\n",
    "\n",
    "for ax, image in zip(axes, batchIMG[0:6]) :\n",
    "    print('Shape after resize : ', image.shape)\n",
    "    ax.imshow(image[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train, path_test, cap_train, cap_test = train_test_split(df_cap['Path'].to_numpy(),\n",
    "                                                              cap_vector,\n",
    "                                                              test_size=0.2,\n",
    "                                                              random_state=101)\n",
    "\n",
    "\n",
    "# df_cap['caption'] phai duoc thay bang vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test.shape, path_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap_test.shape, cap_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training data for images: \" + str(len(path_train)))\n",
    "print(\"Testing data for images: \" + str(len(path_test)))\n",
    "print(\"Training data for Captions: \" + str(len(cap_train)))\n",
    "print(\"Testing data for Captions: \" + str(len(cap_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset_size = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "# print(\"Train dataset size:\", train_dataset_size, f\"(~= {path_train.shape[0]/BATCH_SIZE})\")\n",
    "\n",
    "# test_dataset_size = tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "# print(\"Test dataset size:\", test_dataset_size, f\"(~= {path_test.shape[0]/BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img_features = {}\n",
    "progress_bar = tqdm(img_proc_list, dynamic_ncols=True)\n",
    "for idx, (image, image_path) in enumerate(progress_bar):\n",
    "    batch_features = get_image_features(image)\n",
    "    batch_features = tf.reshape(batch_features, (batch_features.shape[0], -1, batch_features.shape[-1]))\n",
    "    progress_bar.set_description(f\"Processing item: {idx}\")\n",
    "                                     \n",
    "    for batch_feat, path in zip(batch_features, image_path) :\n",
    "        feature_path = path.numpy().decode('utf-8')\n",
    "        img_features[feature_path] = batch_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(img_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list(img_features.keys())[:2]:\n",
    "    print(i)\n",
    "    print(img_features[i])\n",
    "    print('=========')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_feature_map(imagePath, caption):\n",
    "    try:\n",
    "        feature_tensor = img_features[imagePath.decode('utf-8')]\n",
    "    except:\n",
    "        feature_tensor = img_features[imagePath]\n",
    "    return feature_tensor, caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagePath = df_cap['Path'][100]\n",
    "caption_test = df_cap['Captions'][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(image_feature_map(imagePath, caption_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "get = inception_model(load_image(imagePath)[0], training=False)\n",
    "get = tf.reshape(get, (get.shape[0], -1, batch_features.shape[-1]))\n",
    "get[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1000\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "def gen_dataset(img, capt):\n",
    "    data = tf.data.Dataset.from_tensor_slices((img, capt))\n",
    "    \n",
    "    data = data.map(lambda val1, val2 : tf.numpy_function(image_feature_map, [val1, val2], [tf.float32, tf.int32]))\n",
    "    data = data.shuffle(BUFFER_SIZE, reshuffle_each_iteration=True)\n",
    "    data = data.batch(BATCH_SIZE, drop_remainder=False)\n",
    "    data = data.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = gen_dataset(path_train, cap_train)\n",
    "test_dataset = gen_dataset(path_test, cap_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img_batch, sample_cap_batch = next(iter(train_dataset))\n",
    "print(sample_img_batch.shape)\n",
    "print(sample_cap_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_size = tf.data.experimental.cardinality(train_dataset).numpy()\n",
    "print(\"Train dataset size:\", train_dataset_size, f\"(~= {path_train.shape[0]/BATCH_SIZE})\")\n",
    "\n",
    "test_dataset_size = tf.data.experimental.cardinality(test_dataset).numpy()\n",
    "print(\"Test dataset size:\", test_dataset_size, f\"(~= {path_test.shape[0]/BATCH_SIZE})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.reset_default_graph()\n",
    "print(tf.compat.v1.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building\n",
    "1.Set the parameters\n",
    "\n",
    "2.Build the Encoder, Attention model & Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 256 \n",
    "units = 512\n",
    "\n",
    "#top 5,000 words +1\n",
    "vocab_size = 5001\n",
    "train_num_steps = len(path_train) // BATCH_SIZE #len(total train images) // BATCH_SIZE\n",
    "test_num_steps = len(path_test) // BATCH_SIZE  #len(total test images) // BATCH_SIZE\n",
    "\n",
    "max_length = longest_word_length\n",
    "feature_shape = 2048\n",
    "attention_feature_shape = 16 # shape features output of InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(Model):\n",
    "    def __init__(self,embed_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dense = tf.keras.layers.Dense(embed_dim) #build your Dense layer with relu activation\n",
    "        \n",
    "    def call(self, features):\n",
    "        features =  self.dense(features) # extract the features from the image shape: (batch, 8*8, embed_dim)\n",
    "        features =  tf.keras.activations.relu(features, alpha=0.01, max_value=None, threshold=0)\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder=Encoder(embedding_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention_model(Model):\n",
    "    def __init__(self, units):\n",
    "        super(Attention_model, self).__init__()\n",
    "        self.W1 = tf.keras.layers.Dense(units) #build your Dense layer\n",
    "        self.W2 = tf.keras.layers.Dense(units) #build your Dense layer\n",
    "        self.V = tf.keras.layers.Dense(1) #build your final Dense layer with unit 1\n",
    "        self.units=units\n",
    "\n",
    "    def call(self, features, hidden):\n",
    "        hidden_with_time_axis = hidden[:, tf.newaxis]\n",
    "\n",
    "        # build your score funciton to shape: (batch_size, 8*8, units)\n",
    "        score = tf.keras.activations.tanh(self.W1(features) + self.W2(hidden_with_time_axis))  \n",
    "\n",
    "        # extract your attention weights with shape: (batch_size, 8*8, 1)\n",
    "        attention_weights = tf.keras.activations.softmax(self.V(score), axis=1) \n",
    "\n",
    "        #shape: create the context vector with shape (batch_size, 8*8,embedding_dim)\n",
    "        context_vector = attention_weights * features \n",
    "\n",
    "        # reduce the shape to (batch_size, embedding_dim)\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  \n",
    "        \n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(Model):\n",
    "    def __init__(self, embed_dim, units, vocab_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.units=units\n",
    "        self.attention = Attention_model(self.units) #iniitalise your Attention model with units\n",
    "        self.embed = tf.keras.layers.Embedding(vocab_size, embed_dim) #build your Embedding layer\n",
    "        self.gru = tf.keras.layers.GRU(self.units,return_sequences=True,return_state=True,recurrent_initializer='glorot_uniform')\n",
    "        self.d1 = tf.keras.layers.Dense(self.units) #build your Dense layer\n",
    "        self.d2 = tf.keras.layers.Dense(vocab_size) #build your Dense layer\n",
    "        \n",
    "\n",
    "    def call(self,x,features, hidden):\n",
    "        context_vector, attention_weights = self.attention(features, hidden) #create your context vector & attention weights from attention model\n",
    "        embed = self.embed(x) # embed your input to shape: (batch_size, 1, embedding_dim)\n",
    "        embed = tf.concat([tf.expand_dims(context_vector, 1), embed], axis = -1) # Concatenate your input with the context vector from attention layer. Shape: (batch_size, 1, embedding_dim + embedding_dim)\n",
    "        output,state = self.gru(embed) # Extract the output & hidden state from GRU layer. Output shape : (batch_size, max_length, hidden_size)\n",
    "        output = self.d1(output)\n",
    "        output = tf.reshape(output, (-1, output.shape[2])) # shape : (batch_size * max_length, hidden_size)\n",
    "        output = self.d2(output) # shape : (batch_size * max_length, vocab_size)\n",
    "        \n",
    "        return output, state, attention_weights\n",
    "    \n",
    "    def init_state(self, batch_size):\n",
    "        return tf.zeros((batch_size, self.units))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder=Decoder(embedding_dim, units, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features=encoder(sample_img_batch)\n",
    "\n",
    "hidden = decoder.init_state(batch_size=sample_cap_batch.shape[0])\n",
    "dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * sample_cap_batch.shape[0], 1)\n",
    "\n",
    "predictions, hidden_out, attention_weights= decoder(dec_input, features, hidden)\n",
    "print('Feature shape from Encoder: {}'.format(features.shape)) #(batch, 8*8, embed_dim)\n",
    "print('Predcitions shape from Decoder: {}'.format(predictions.shape)) #(batch,vocab_size)\n",
    "print('Attention weights shape from Decoder: {}'.format(attention_weights.shape)) #(batch, 8*8, embed_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training & optimization\n",
    "1.Set the optimizer & loss object\n",
    "\n",
    "2.Create your checkpoint path\n",
    "\n",
    "3.Create your training & testing step functions\n",
    "\n",
    "4.Create your loss function for the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True, reduction = tf.keras.losses.Reduction.NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    #loss is getting multiplied with mask to get an ideal shape\n",
    "    \n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"eye_blind/ckpt\"\n",
    "ckpt = tf.train.Checkpoint(encoder=encoder,\n",
    "                           decoder=decoder,\n",
    "                           optimizer = optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    start_epoch = int(ckpt_manager.latest_checkpoint.split('-')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While creating the training step for your model, you will apply Teacher forcing.\n",
    "* Teacher forcing is a technique where the target/real word is passed as the next input to the decoder instead of previous prediciton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    hidden = decoder.init_state(batch_size=target.shape[0])\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_op = encoder(img_tensor)\n",
    "\n",
    "        for r in range(1, target.shape[1]) :\n",
    "            predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
    "            loss = loss + loss_function(target[:, r], predictions) \n",
    "            dec_input = tf.expand_dims(target[:, r], 1)  \n",
    "\n",
    "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
    "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars))\n",
    "\n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* While creating the test step for your model, you will pass your previous prediciton as the next input to the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(img_tensor, target):\n",
    "    loss = 0\n",
    "    \n",
    "    #write your code here to do the testing steps\n",
    "    hidden = decoder.init_state(batch_size = target.shape[0])\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']] * target.shape[0], 1)\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_op = encoder(img_tensor)\n",
    "\n",
    "        #apply teacher forcing again\n",
    "        for r in range(1, target.shape[1]) :\n",
    "        #pass encoder_op to decoder\n",
    "            predictions, hidden, _ = decoder(dec_input, encoder_op, hidden)\n",
    "            loss = loss + loss_function(target[:, r], predictions)\n",
    "\n",
    "            dec_input = tf.expand_dims(target[: , r], 1)\n",
    "\n",
    "    avg_loss = (loss/ int(target.shape[1])) #avg loss per batch\n",
    "    trainable_vars = encoder.trainable_variables + decoder.trainable_variables\n",
    "    grad = tape.gradient (loss, trainable_vars) # calculating gradient wrt each trainable var\n",
    "\n",
    "    #we will now compute the gradients and apply it to the optimizer while backpropagating\n",
    "    optimizer.apply_gradients(zip(grad, trainable_vars))                      \n",
    "\n",
    "\n",
    "    return loss, avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_loss_cal(test_dataset):\n",
    "    total_loss = 0\n",
    "\n",
    "    #write your code to get the average loss result on your test data\n",
    "    for (batch, (img_tensor, target)) in enumerate(test_dataset) :\n",
    "        batch_loss, t_loss = test_step(img_tensor, target)\n",
    "        total_loss = total_loss + t_loss\n",
    "        avg_test_loss = total_loss/ test_num_steps\n",
    "\n",
    "    return avg_test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_plot = []\n",
    "test_loss_plot = []\n",
    "EPOCHS = 15\n",
    "\n",
    "best_test_loss=100\n",
    "for epoch in tqdm(range(0, EPOCHS)):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (img_tensor, target)) in enumerate(train_dataset):\n",
    "        batch_loss, t_loss = train_step(img_tensor, target)\n",
    "        total_loss += t_loss\n",
    "        avg_train_loss=total_loss / train_num_steps\n",
    "        \n",
    "    loss_plot.append(avg_train_loss)    \n",
    "    test_loss = test_loss_cal(test_dataset)\n",
    "    test_loss_plot.append(test_loss)\n",
    "    \n",
    "    print ('For epoch: {}, the train loss is {:.3f}, & test loss is {:.3f}'.format(epoch+1,avg_train_loss,test_loss))\n",
    "    print ('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "    \n",
    "    if test_loss < best_test_loss:\n",
    "        print('Test loss has been reduced from %.3f to %.3f' % (best_test_loss, test_loss))\n",
    "        best_test_loss = test_loss\n",
    "        ckpt_manager.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(loss_plot)\n",
    "plt.plot(test_loss_plot)\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: \n",
    "* Since there is a difference between the train & test steps ( Presence of teacher forcing), you may observe that the train loss is decreasing while your test loss is not. \n",
    "* This doesn't mean that the model is overfitting, as we can't compare the train & test results here, as both approach is different.\n",
    "* Also, if you want to achieve better results you can run it more epochs, but the intent of this capstone is to give you an idea on how to integrate attention mechanism with E-D architecture for images. The intent is not to create the state of art model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "1.Define your evaluation function using greedy search\n",
    "\n",
    "2.Define your evaluation function using beam search ( optional)\n",
    "\n",
    "3.Test it on a sample data using BLEU score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greedy Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(image):\n",
    "    attention_plot = np.zeros((max_length, attention_features_shape))\n",
    "\n",
    "    hidden = decoder.reset_state(batch_size=1)\n",
    "\n",
    "    temp_input = tf.expand_dims(load_image(image)[0], 0) #process the input image to desired format before extracting features\n",
    "    img_tensor_val = # Extract features using our feature extraction model\n",
    "    img_tensor_val = tf.reshape(img_tensor_val, (img_tensor_val.shape[0], -1, img_tensor_val.shape[3]))\n",
    "\n",
    "    features = # extract the features by passing the input to encoder\n",
    "\n",
    "    dec_input = tf.expand_dims([tokenizer.word_index['<start>']], 0)\n",
    "    result = []\n",
    "\n",
    "    for i in range(max_length):\n",
    "        predictions, hidden, attention_weights = # get the output from decoder\n",
    "\n",
    "        attention_plot[i] = tf.reshape(attention_weights, (-1, )).numpy()\n",
    "\n",
    "        predicted_id = #extract the predicted id(embedded value) which carries the max value\n",
    "        #map the id to the word from tokenizer and append the value to the result list\n",
    "\n",
    "        if tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, attention_plot,predictions\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    attention_plot = attention_plot[:len(result), :]\n",
    "    return result, attention_plot,predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beam Search(optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_evaluate(image, beam_index = #your value for beam index):\n",
    "\n",
    "    #write your code to evaluate the result using beam search\n",
    "                  \n",
    "    return final_caption\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attmap(caption, weights, image):\n",
    "\n",
    "    fig = plt.figure(figsize=(10, 10))\n",
    "    temp_img = np.array(Image.open(image))\n",
    "    \n",
    "    len_cap = len(caption)\n",
    "    for cap in range(len_cap):\n",
    "        weights_img = np.reshape(weights[cap], (8,8))\n",
    "        weights_img = np.array(Image.fromarray(weights_img).resize((224, 224), Image.LANCZOS))\n",
    "        \n",
    "        ax = fig.add_subplot(len_cap//2, len_cap//2, cap+1)\n",
    "        ax.set_title(caption[cap], fontsize=15)\n",
    "        \n",
    "        img=ax.imshow(temp_img)\n",
    "        \n",
    "        ax.imshow(weights_img, cmap='gist_heat', alpha=0.6,extent=img.get_extent())\n",
    "        ax.axis('off')\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0.2)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_text(text):\n",
    "    filt=['<start>','<unk>','<end>'] \n",
    "    temp= text.split()\n",
    "    [temp.remove(j) for k in filt for j in temp if k==j]\n",
    "    text=' '.join(temp)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rid = np.random.randint(0, len(img_test))\n",
    "test_image = img_test[rid]\n",
    "#test_image = './images/413231421_43833a11f5.jpg'\n",
    "#real_caption = '<start> black dog is digging in the snow <end>'\n",
    "\n",
    "real_caption = ' '.join([tokenizer.index_word[i] for i in cap_test[rid] if i not in [0]])\n",
    "result, attention_plot,pred_test = evaluate(test_image)\n",
    "\n",
    "\n",
    "real_caption=filt_text(real_caption)      \n",
    "\n",
    "\n",
    "pred_caption=' '.join(result).rsplit(' ', 1)[0]\n",
    "\n",
    "real_appn = []\n",
    "real_appn.append(real_caption.split())\n",
    "reference = real_appn\n",
    "candidate = pred_caption.split()\n",
    "\n",
    "score = sentence_bleu(reference, candidate, weights=#set your weights)\n",
    "print(f\"BELU score: {score*100}\")\n",
    "\n",
    "print ('Real Caption:', real_caption)\n",
    "print ('Prediction Caption:', pred_caption)\n",
    "plot_attmap(result, attention_plot, test_image)\n",
    "\n",
    "\n",
    "Image.open(test_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "captions=beam_evaluate(test_image)\n",
    "print(captions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
